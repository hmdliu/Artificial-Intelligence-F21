{"cells":[{"cell_type":"markdown","metadata":{"id":"0Pteazlue9LE"},"source":["<h2><center> CSCI - UA 9472 - Artificial Intelligence </center></h2>\n","\n","<h3><center> Assignment 4: Reinforcement Learning </center></h3>\n","\n","<center>Given date: December 7\n","</center>\n","<center><font color='red'>Due date: December 20/21 </font>\n","</center>\n","<center><b>Total: 20 pts </b>\n","</center>\n"]},{"cell_type":"markdown","metadata":{"id":"0pnrzYsFe9LP"},"source":["<center>In this fourth assignment, we will continue our quest for the optimal agent and replace our heavy logical reasoning model with a faster implementation based on Q-learning</center>"]},{"cell_type":"markdown","metadata":{"id":"kM8ImLvJe9LQ"},"source":["<img src=\"https://github.com/acosse/Artificial-Intelligence-Fall2021/blob/main/Assignments/Assignment4/MonsterKill.png?raw=1\" width=\"400\" height=\"300\"/>\n","\n"," = = = = = = = = = = = = = = = = = = = = = \\\n"," **Name:** Haoming(Hammond) Liu \\\n"," **NetID:** hl3797 \\\n"," **Date:** Dec. 20th, 2021 \\\n","  = = = = = = = = = = = = = = = = = = = = ="]},{"cell_type":"code","source":["import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow import keras\n","from keras.layers import Dense\n","from keras.models import Sequential"],"metadata":{"id":"cP03yBFTgOL3","executionInfo":{"status":"ok","timestamp":1640108897901,"user_tz":300,"elapsed":6676,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bHNFk-te9LQ"},"source":["#### Question 1. A simple moving agent (10pts)\n","\n","We consider a simple environment as shown below. Our simple agent has the possibility to move West, East, North and South (except on the borders of the environment). Moreover it has two additional actions:\n","    \n","   - It can hit with its sword (which is essentially useful when facing the skeleton)\n","    \n","   - It can open the door and escape the room (useful when being in the upper rightmost cell)\n","   \n","As you can imagine, hitting while in an empty cell is a loss of energy and should be penalized. We will thus associate useless behaviors (like hitting when alone and opening non existing doors) with a negative reward of -10. Hitting the skeleton and opening the door _when the skeleton has been killed_ should be associated with higher rewards of +30. Escaping without the kill will be associated with a negative reward of -10 (The agent should not exit before getting rid of all the evil in the room)\n","\n","There are a total of 48 cells. Each cell can contain the skeleton, be empty or contain the door (exclusively) which makes a total of $48\\times3 = 144$ states for your environment. Moreover, we want to keep track of whether or not the skeleton has been killed so we will take into account an additional variable encoding whether the kill has been completed. This thus leads to a total of $48\\times 3\\times2 = 288$.\n","\n","Any move will be associated with a small penalty of -1 point    "]},{"cell_type":"markdown","metadata":{"id":"lM8fLVBKe9LR"},"source":["<img src=\"https://github.com/acosse/Artificial-Intelligence-Fall2021/blob/main/Assignments/Assignment4/mazeAssignment3a.png?raw=1\" width=\"400\" height=\"300\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"5-FfAz2we9LS"},"source":["Start by coding a simple TD Q-learning agent which stores the Q-table entirely as a $288\\times 6$ numpy array. Use an exploration function to avoid being stuck in a limited region of the environment. Recall that such a function can be defined as \n","\n","\\begin{align}\n","f(Q_\\theta[s,a], N[s,a]) = \\left\\{\\begin{array}{ll}\n","R^+& \\text{if $N[s,a]<N_e$}\\\\\n","Q_\\theta[s,a]&\\text{otherwise}\n","\\end{array}\\right. \n","\\end{align}\n","\n","where $R^+$ is an estimate of the best possible reward that can be obtained in any state.\n","\n","Concretely for each new state and action, you should update the $Q$-table after incrementing the count $N[s,a]$ (of the number of times action $a$ has been taken in state $s$) \n","\n","\\begin{align}\n","Q[s, a] & \\leftarrow Q[s,a] +\\eta \\left(N_{sa}[s,a]\\right)\\left(r+\\gamma \\max_{a'} Q[s',a'] - Q[s,a]\\right)\\\\\n","s,a,r &\\leftarrow s', \\underset{a'}{\\operatorname{argmax}} f\\left(Q[s',a'], N[s',a']\\right), r'\n","\\end{align}\n","\n","\\eta \\left(N_{sa}[s,a]\\right) is a learning rate that can be set to a sufficiently small constant or be evolving as the inverse of $N[s, a]$ \n","\n","Consider a couple of episodes to make sure that you fill out a sufficient number of entries in the table. Interlace the learning phases with a couple of evaluation phases during which you should store the number of steps needed to complete the kill and exit so that you will be able to plot the evolution of your agent at the end of the simulation. \n","\n","You can associate a negative reward with the impossible actions corresponding to hitting walls on the borders of the environment."]},{"cell_type":"code","source":["\n","# init params\n","Ne = 3\n","door_pos = (7, 5)\n","skeleton_pos = (3, 3)\n","\n","# exploration function\n","exp_func = lambda n, q: float('inf') if n < Ne else q\n","\n","# step to the next state\n","def step(s, a, bound_punish=-2):\n","\n","    # init rewards, action, and states\n","    rewards = -1\n","    x, y, cell, skeleton = idx_to_state[s]\n","\n","    # assign rewards and new states\n","    if actions[a] == 'left':\n","        if x == 0: rewards += bound_punish\n","        else: x -= 1\n","    if actions[a] == 'right':\n","        if x == 7: rewards += bound_punish\n","        else: x += 1\n","    if actions[a] == 'up':\n","        if y == 5: rewards += bound_punish\n","        else: y += 1\n","    if actions[a] == 'down':\n","        if y == 0: rewards += bound_punish\n","        else: y -= 1\n","    if actions[a] == 'hit':\n","        if (x, y) == skeleton_pos and skeleton:\n","            skeleton, rewards = False, 30\n","        else:\n","            rewards -= 10\n","    if actions[a] == 'leave':\n","        if (x, y) == door_pos and not skeleton:\n","            return 'exit', 30\n","        else:\n","            rewards -= 10\n","    \n","    # check state of new cell\n","    cell = 'nothing'\n","    if (x, y) == door_pos:\n","        cell = 'door'\n","    if (x, y) == skeleton_pos:\n","        cell = 'skeleton'\n","\n","    return state_to_idx[x, y, cell, skeleton], rewards\n","\n","# test the 'step' function\n","def test_step(state, action):\n","    a = actions.index(action)\n","    s, r = step(state_to_idx[state], a)\n","    print('\\nTest Step:', action, 'at', state)\n","    if s == 'exit':\n","        print('game end!')\n","        return\n","    print('result:', idx_to_state[s], 'with reward', r)\n","\n","# map states to index [0, 287]\n","idx = 0\n","idx_to_state = []\n","state_to_idx = {}\n","actions = ['left', 'right', 'up', 'down', 'hit', 'leave']\n","cell_types = ['nothing', 'skeleton', 'door']\n","for i in range(8):\n","    for j in range(6):\n","        for k in cell_types:\n","            idx_to_state.append((i, j, k, True))\n","            idx_to_state.append((i, j, k, False))\n","            state_to_idx[i, j, k, True] = idx\n","            state_to_idx[i, j, k, False] = idx + 1\n","            idx += 2\n","\n","# test printing\n","print(idx_to_state)\n","print(state_to_idx)\n","test_step((0, 0, 'nothing', True), 'left')\n","test_step((3, 3, 'skeleton', True), 'hit')\n","test_step((7, 5, 'door', False), 'leave')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcEk2X4BhYae","executionInfo":{"status":"ok","timestamp":1640108898088,"user_tz":300,"elapsed":211,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"77ec5822-4919-4a65-967b-982d9fcf6137"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0, 'nothing', True), (0, 0, 'nothing', False), (0, 0, 'skeleton', True), (0, 0, 'skeleton', False), (0, 0, 'door', True), (0, 0, 'door', False), (0, 1, 'nothing', True), (0, 1, 'nothing', False), (0, 1, 'skeleton', True), (0, 1, 'skeleton', False), (0, 1, 'door', True), (0, 1, 'door', False), (0, 2, 'nothing', True), (0, 2, 'nothing', False), (0, 2, 'skeleton', True), (0, 2, 'skeleton', False), (0, 2, 'door', True), (0, 2, 'door', False), (0, 3, 'nothing', True), (0, 3, 'nothing', False), (0, 3, 'skeleton', True), (0, 3, 'skeleton', False), (0, 3, 'door', True), (0, 3, 'door', False), (0, 4, 'nothing', True), (0, 4, 'nothing', False), (0, 4, 'skeleton', True), (0, 4, 'skeleton', False), (0, 4, 'door', True), (0, 4, 'door', False), (0, 5, 'nothing', True), (0, 5, 'nothing', False), (0, 5, 'skeleton', True), (0, 5, 'skeleton', False), (0, 5, 'door', True), (0, 5, 'door', False), (1, 0, 'nothing', True), (1, 0, 'nothing', False), (1, 0, 'skeleton', True), (1, 0, 'skeleton', False), (1, 0, 'door', True), (1, 0, 'door', False), (1, 1, 'nothing', True), (1, 1, 'nothing', False), (1, 1, 'skeleton', True), (1, 1, 'skeleton', False), (1, 1, 'door', True), (1, 1, 'door', False), (1, 2, 'nothing', True), (1, 2, 'nothing', False), (1, 2, 'skeleton', True), (1, 2, 'skeleton', False), (1, 2, 'door', True), (1, 2, 'door', False), (1, 3, 'nothing', True), (1, 3, 'nothing', False), (1, 3, 'skeleton', True), (1, 3, 'skeleton', False), (1, 3, 'door', True), (1, 3, 'door', False), (1, 4, 'nothing', True), (1, 4, 'nothing', False), (1, 4, 'skeleton', True), (1, 4, 'skeleton', False), (1, 4, 'door', True), (1, 4, 'door', False), (1, 5, 'nothing', True), (1, 5, 'nothing', False), (1, 5, 'skeleton', True), (1, 5, 'skeleton', False), (1, 5, 'door', True), (1, 5, 'door', False), (2, 0, 'nothing', True), (2, 0, 'nothing', False), (2, 0, 'skeleton', True), (2, 0, 'skeleton', False), (2, 0, 'door', True), (2, 0, 'door', False), (2, 1, 'nothing', True), (2, 1, 'nothing', False), (2, 1, 'skeleton', True), (2, 1, 'skeleton', False), (2, 1, 'door', True), (2, 1, 'door', False), (2, 2, 'nothing', True), (2, 2, 'nothing', False), (2, 2, 'skeleton', True), (2, 2, 'skeleton', False), (2, 2, 'door', True), (2, 2, 'door', False), (2, 3, 'nothing', True), (2, 3, 'nothing', False), (2, 3, 'skeleton', True), (2, 3, 'skeleton', False), (2, 3, 'door', True), (2, 3, 'door', False), (2, 4, 'nothing', True), (2, 4, 'nothing', False), (2, 4, 'skeleton', True), (2, 4, 'skeleton', False), (2, 4, 'door', True), (2, 4, 'door', False), (2, 5, 'nothing', True), (2, 5, 'nothing', False), (2, 5, 'skeleton', True), (2, 5, 'skeleton', False), (2, 5, 'door', True), (2, 5, 'door', False), (3, 0, 'nothing', True), (3, 0, 'nothing', False), (3, 0, 'skeleton', True), (3, 0, 'skeleton', False), (3, 0, 'door', True), (3, 0, 'door', False), (3, 1, 'nothing', True), (3, 1, 'nothing', False), (3, 1, 'skeleton', True), (3, 1, 'skeleton', False), (3, 1, 'door', True), (3, 1, 'door', False), (3, 2, 'nothing', True), (3, 2, 'nothing', False), (3, 2, 'skeleton', True), (3, 2, 'skeleton', False), (3, 2, 'door', True), (3, 2, 'door', False), (3, 3, 'nothing', True), (3, 3, 'nothing', False), (3, 3, 'skeleton', True), (3, 3, 'skeleton', False), (3, 3, 'door', True), (3, 3, 'door', False), (3, 4, 'nothing', True), (3, 4, 'nothing', False), (3, 4, 'skeleton', True), (3, 4, 'skeleton', False), (3, 4, 'door', True), (3, 4, 'door', False), (3, 5, 'nothing', True), (3, 5, 'nothing', False), (3, 5, 'skeleton', True), (3, 5, 'skeleton', False), (3, 5, 'door', True), (3, 5, 'door', False), (4, 0, 'nothing', True), (4, 0, 'nothing', False), (4, 0, 'skeleton', True), (4, 0, 'skeleton', False), (4, 0, 'door', True), (4, 0, 'door', False), (4, 1, 'nothing', True), (4, 1, 'nothing', False), (4, 1, 'skeleton', True), (4, 1, 'skeleton', False), (4, 1, 'door', True), (4, 1, 'door', False), (4, 2, 'nothing', True), (4, 2, 'nothing', False), (4, 2, 'skeleton', True), (4, 2, 'skeleton', False), (4, 2, 'door', True), (4, 2, 'door', False), (4, 3, 'nothing', True), (4, 3, 'nothing', False), (4, 3, 'skeleton', True), (4, 3, 'skeleton', False), (4, 3, 'door', True), (4, 3, 'door', False), (4, 4, 'nothing', True), (4, 4, 'nothing', False), (4, 4, 'skeleton', True), (4, 4, 'skeleton', False), (4, 4, 'door', True), (4, 4, 'door', False), (4, 5, 'nothing', True), (4, 5, 'nothing', False), (4, 5, 'skeleton', True), (4, 5, 'skeleton', False), (4, 5, 'door', True), (4, 5, 'door', False), (5, 0, 'nothing', True), (5, 0, 'nothing', False), (5, 0, 'skeleton', True), (5, 0, 'skeleton', False), (5, 0, 'door', True), (5, 0, 'door', False), (5, 1, 'nothing', True), (5, 1, 'nothing', False), (5, 1, 'skeleton', True), (5, 1, 'skeleton', False), (5, 1, 'door', True), (5, 1, 'door', False), (5, 2, 'nothing', True), (5, 2, 'nothing', False), (5, 2, 'skeleton', True), (5, 2, 'skeleton', False), (5, 2, 'door', True), (5, 2, 'door', False), (5, 3, 'nothing', True), (5, 3, 'nothing', False), (5, 3, 'skeleton', True), (5, 3, 'skeleton', False), (5, 3, 'door', True), (5, 3, 'door', False), (5, 4, 'nothing', True), (5, 4, 'nothing', False), (5, 4, 'skeleton', True), (5, 4, 'skeleton', False), (5, 4, 'door', True), (5, 4, 'door', False), (5, 5, 'nothing', True), (5, 5, 'nothing', False), (5, 5, 'skeleton', True), (5, 5, 'skeleton', False), (5, 5, 'door', True), (5, 5, 'door', False), (6, 0, 'nothing', True), (6, 0, 'nothing', False), (6, 0, 'skeleton', True), (6, 0, 'skeleton', False), (6, 0, 'door', True), (6, 0, 'door', False), (6, 1, 'nothing', True), (6, 1, 'nothing', False), (6, 1, 'skeleton', True), (6, 1, 'skeleton', False), (6, 1, 'door', True), (6, 1, 'door', False), (6, 2, 'nothing', True), (6, 2, 'nothing', False), (6, 2, 'skeleton', True), (6, 2, 'skeleton', False), (6, 2, 'door', True), (6, 2, 'door', False), (6, 3, 'nothing', True), (6, 3, 'nothing', False), (6, 3, 'skeleton', True), (6, 3, 'skeleton', False), (6, 3, 'door', True), (6, 3, 'door', False), (6, 4, 'nothing', True), (6, 4, 'nothing', False), (6, 4, 'skeleton', True), (6, 4, 'skeleton', False), (6, 4, 'door', True), (6, 4, 'door', False), (6, 5, 'nothing', True), (6, 5, 'nothing', False), (6, 5, 'skeleton', True), (6, 5, 'skeleton', False), (6, 5, 'door', True), (6, 5, 'door', False), (7, 0, 'nothing', True), (7, 0, 'nothing', False), (7, 0, 'skeleton', True), (7, 0, 'skeleton', False), (7, 0, 'door', True), (7, 0, 'door', False), (7, 1, 'nothing', True), (7, 1, 'nothing', False), (7, 1, 'skeleton', True), (7, 1, 'skeleton', False), (7, 1, 'door', True), (7, 1, 'door', False), (7, 2, 'nothing', True), (7, 2, 'nothing', False), (7, 2, 'skeleton', True), (7, 2, 'skeleton', False), (7, 2, 'door', True), (7, 2, 'door', False), (7, 3, 'nothing', True), (7, 3, 'nothing', False), (7, 3, 'skeleton', True), (7, 3, 'skeleton', False), (7, 3, 'door', True), (7, 3, 'door', False), (7, 4, 'nothing', True), (7, 4, 'nothing', False), (7, 4, 'skeleton', True), (7, 4, 'skeleton', False), (7, 4, 'door', True), (7, 4, 'door', False), (7, 5, 'nothing', True), (7, 5, 'nothing', False), (7, 5, 'skeleton', True), (7, 5, 'skeleton', False), (7, 5, 'door', True), (7, 5, 'door', False)]\n","{(0, 0, 'nothing', True): 0, (0, 0, 'nothing', False): 1, (0, 0, 'skeleton', True): 2, (0, 0, 'skeleton', False): 3, (0, 0, 'door', True): 4, (0, 0, 'door', False): 5, (0, 1, 'nothing', True): 6, (0, 1, 'nothing', False): 7, (0, 1, 'skeleton', True): 8, (0, 1, 'skeleton', False): 9, (0, 1, 'door', True): 10, (0, 1, 'door', False): 11, (0, 2, 'nothing', True): 12, (0, 2, 'nothing', False): 13, (0, 2, 'skeleton', True): 14, (0, 2, 'skeleton', False): 15, (0, 2, 'door', True): 16, (0, 2, 'door', False): 17, (0, 3, 'nothing', True): 18, (0, 3, 'nothing', False): 19, (0, 3, 'skeleton', True): 20, (0, 3, 'skeleton', False): 21, (0, 3, 'door', True): 22, (0, 3, 'door', False): 23, (0, 4, 'nothing', True): 24, (0, 4, 'nothing', False): 25, (0, 4, 'skeleton', True): 26, (0, 4, 'skeleton', False): 27, (0, 4, 'door', True): 28, (0, 4, 'door', False): 29, (0, 5, 'nothing', True): 30, (0, 5, 'nothing', False): 31, (0, 5, 'skeleton', True): 32, (0, 5, 'skeleton', False): 33, (0, 5, 'door', True): 34, (0, 5, 'door', False): 35, (1, 0, 'nothing', True): 36, (1, 0, 'nothing', False): 37, (1, 0, 'skeleton', True): 38, (1, 0, 'skeleton', False): 39, (1, 0, 'door', True): 40, (1, 0, 'door', False): 41, (1, 1, 'nothing', True): 42, (1, 1, 'nothing', False): 43, (1, 1, 'skeleton', True): 44, (1, 1, 'skeleton', False): 45, (1, 1, 'door', True): 46, (1, 1, 'door', False): 47, (1, 2, 'nothing', True): 48, (1, 2, 'nothing', False): 49, (1, 2, 'skeleton', True): 50, (1, 2, 'skeleton', False): 51, (1, 2, 'door', True): 52, (1, 2, 'door', False): 53, (1, 3, 'nothing', True): 54, (1, 3, 'nothing', False): 55, (1, 3, 'skeleton', True): 56, (1, 3, 'skeleton', False): 57, (1, 3, 'door', True): 58, (1, 3, 'door', False): 59, (1, 4, 'nothing', True): 60, (1, 4, 'nothing', False): 61, (1, 4, 'skeleton', True): 62, (1, 4, 'skeleton', False): 63, (1, 4, 'door', True): 64, (1, 4, 'door', False): 65, (1, 5, 'nothing', True): 66, (1, 5, 'nothing', False): 67, (1, 5, 'skeleton', True): 68, (1, 5, 'skeleton', False): 69, (1, 5, 'door', True): 70, (1, 5, 'door', False): 71, (2, 0, 'nothing', True): 72, (2, 0, 'nothing', False): 73, (2, 0, 'skeleton', True): 74, (2, 0, 'skeleton', False): 75, (2, 0, 'door', True): 76, (2, 0, 'door', False): 77, (2, 1, 'nothing', True): 78, (2, 1, 'nothing', False): 79, (2, 1, 'skeleton', True): 80, (2, 1, 'skeleton', False): 81, (2, 1, 'door', True): 82, (2, 1, 'door', False): 83, (2, 2, 'nothing', True): 84, (2, 2, 'nothing', False): 85, (2, 2, 'skeleton', True): 86, (2, 2, 'skeleton', False): 87, (2, 2, 'door', True): 88, (2, 2, 'door', False): 89, (2, 3, 'nothing', True): 90, (2, 3, 'nothing', False): 91, (2, 3, 'skeleton', True): 92, (2, 3, 'skeleton', False): 93, (2, 3, 'door', True): 94, (2, 3, 'door', False): 95, (2, 4, 'nothing', True): 96, (2, 4, 'nothing', False): 97, (2, 4, 'skeleton', True): 98, (2, 4, 'skeleton', False): 99, (2, 4, 'door', True): 100, (2, 4, 'door', False): 101, (2, 5, 'nothing', True): 102, (2, 5, 'nothing', False): 103, (2, 5, 'skeleton', True): 104, (2, 5, 'skeleton', False): 105, (2, 5, 'door', True): 106, (2, 5, 'door', False): 107, (3, 0, 'nothing', True): 108, (3, 0, 'nothing', False): 109, (3, 0, 'skeleton', True): 110, (3, 0, 'skeleton', False): 111, (3, 0, 'door', True): 112, (3, 0, 'door', False): 113, (3, 1, 'nothing', True): 114, (3, 1, 'nothing', False): 115, (3, 1, 'skeleton', True): 116, (3, 1, 'skeleton', False): 117, (3, 1, 'door', True): 118, (3, 1, 'door', False): 119, (3, 2, 'nothing', True): 120, (3, 2, 'nothing', False): 121, (3, 2, 'skeleton', True): 122, (3, 2, 'skeleton', False): 123, (3, 2, 'door', True): 124, (3, 2, 'door', False): 125, (3, 3, 'nothing', True): 126, (3, 3, 'nothing', False): 127, (3, 3, 'skeleton', True): 128, (3, 3, 'skeleton', False): 129, (3, 3, 'door', True): 130, (3, 3, 'door', False): 131, (3, 4, 'nothing', True): 132, (3, 4, 'nothing', False): 133, (3, 4, 'skeleton', True): 134, (3, 4, 'skeleton', False): 135, (3, 4, 'door', True): 136, (3, 4, 'door', False): 137, (3, 5, 'nothing', True): 138, (3, 5, 'nothing', False): 139, (3, 5, 'skeleton', True): 140, (3, 5, 'skeleton', False): 141, (3, 5, 'door', True): 142, (3, 5, 'door', False): 143, (4, 0, 'nothing', True): 144, (4, 0, 'nothing', False): 145, (4, 0, 'skeleton', True): 146, (4, 0, 'skeleton', False): 147, (4, 0, 'door', True): 148, (4, 0, 'door', False): 149, (4, 1, 'nothing', True): 150, (4, 1, 'nothing', False): 151, (4, 1, 'skeleton', True): 152, (4, 1, 'skeleton', False): 153, (4, 1, 'door', True): 154, (4, 1, 'door', False): 155, (4, 2, 'nothing', True): 156, (4, 2, 'nothing', False): 157, (4, 2, 'skeleton', True): 158, (4, 2, 'skeleton', False): 159, (4, 2, 'door', True): 160, (4, 2, 'door', False): 161, (4, 3, 'nothing', True): 162, (4, 3, 'nothing', False): 163, (4, 3, 'skeleton', True): 164, (4, 3, 'skeleton', False): 165, (4, 3, 'door', True): 166, (4, 3, 'door', False): 167, (4, 4, 'nothing', True): 168, (4, 4, 'nothing', False): 169, (4, 4, 'skeleton', True): 170, (4, 4, 'skeleton', False): 171, (4, 4, 'door', True): 172, (4, 4, 'door', False): 173, (4, 5, 'nothing', True): 174, (4, 5, 'nothing', False): 175, (4, 5, 'skeleton', True): 176, (4, 5, 'skeleton', False): 177, (4, 5, 'door', True): 178, (4, 5, 'door', False): 179, (5, 0, 'nothing', True): 180, (5, 0, 'nothing', False): 181, (5, 0, 'skeleton', True): 182, (5, 0, 'skeleton', False): 183, (5, 0, 'door', True): 184, (5, 0, 'door', False): 185, (5, 1, 'nothing', True): 186, (5, 1, 'nothing', False): 187, (5, 1, 'skeleton', True): 188, (5, 1, 'skeleton', False): 189, (5, 1, 'door', True): 190, (5, 1, 'door', False): 191, (5, 2, 'nothing', True): 192, (5, 2, 'nothing', False): 193, (5, 2, 'skeleton', True): 194, (5, 2, 'skeleton', False): 195, (5, 2, 'door', True): 196, (5, 2, 'door', False): 197, (5, 3, 'nothing', True): 198, (5, 3, 'nothing', False): 199, (5, 3, 'skeleton', True): 200, (5, 3, 'skeleton', False): 201, (5, 3, 'door', True): 202, (5, 3, 'door', False): 203, (5, 4, 'nothing', True): 204, (5, 4, 'nothing', False): 205, (5, 4, 'skeleton', True): 206, (5, 4, 'skeleton', False): 207, (5, 4, 'door', True): 208, (5, 4, 'door', False): 209, (5, 5, 'nothing', True): 210, (5, 5, 'nothing', False): 211, (5, 5, 'skeleton', True): 212, (5, 5, 'skeleton', False): 213, (5, 5, 'door', True): 214, (5, 5, 'door', False): 215, (6, 0, 'nothing', True): 216, (6, 0, 'nothing', False): 217, (6, 0, 'skeleton', True): 218, (6, 0, 'skeleton', False): 219, (6, 0, 'door', True): 220, (6, 0, 'door', False): 221, (6, 1, 'nothing', True): 222, (6, 1, 'nothing', False): 223, (6, 1, 'skeleton', True): 224, (6, 1, 'skeleton', False): 225, (6, 1, 'door', True): 226, (6, 1, 'door', False): 227, (6, 2, 'nothing', True): 228, (6, 2, 'nothing', False): 229, (6, 2, 'skeleton', True): 230, (6, 2, 'skeleton', False): 231, (6, 2, 'door', True): 232, (6, 2, 'door', False): 233, (6, 3, 'nothing', True): 234, (6, 3, 'nothing', False): 235, (6, 3, 'skeleton', True): 236, (6, 3, 'skeleton', False): 237, (6, 3, 'door', True): 238, (6, 3, 'door', False): 239, (6, 4, 'nothing', True): 240, (6, 4, 'nothing', False): 241, (6, 4, 'skeleton', True): 242, (6, 4, 'skeleton', False): 243, (6, 4, 'door', True): 244, (6, 4, 'door', False): 245, (6, 5, 'nothing', True): 246, (6, 5, 'nothing', False): 247, (6, 5, 'skeleton', True): 248, (6, 5, 'skeleton', False): 249, (6, 5, 'door', True): 250, (6, 5, 'door', False): 251, (7, 0, 'nothing', True): 252, (7, 0, 'nothing', False): 253, (7, 0, 'skeleton', True): 254, (7, 0, 'skeleton', False): 255, (7, 0, 'door', True): 256, (7, 0, 'door', False): 257, (7, 1, 'nothing', True): 258, (7, 1, 'nothing', False): 259, (7, 1, 'skeleton', True): 260, (7, 1, 'skeleton', False): 261, (7, 1, 'door', True): 262, (7, 1, 'door', False): 263, (7, 2, 'nothing', True): 264, (7, 2, 'nothing', False): 265, (7, 2, 'skeleton', True): 266, (7, 2, 'skeleton', False): 267, (7, 2, 'door', True): 268, (7, 2, 'door', False): 269, (7, 3, 'nothing', True): 270, (7, 3, 'nothing', False): 271, (7, 3, 'skeleton', True): 272, (7, 3, 'skeleton', False): 273, (7, 3, 'door', True): 274, (7, 3, 'door', False): 275, (7, 4, 'nothing', True): 276, (7, 4, 'nothing', False): 277, (7, 4, 'skeleton', True): 278, (7, 4, 'skeleton', False): 279, (7, 4, 'door', True): 280, (7, 4, 'door', False): 281, (7, 5, 'nothing', True): 282, (7, 5, 'nothing', False): 283, (7, 5, 'skeleton', True): 284, (7, 5, 'skeleton', False): 285, (7, 5, 'door', True): 286, (7, 5, 'door', False): 287}\n","\n","Test Step: left at (0, 0, 'nothing', True)\n","result: (0, 0, 'nothing', True) with reward -3\n","\n","Test Step: hit at (3, 3, 'skeleton', True)\n","result: (3, 3, 'skeleton', False) with reward 30\n","\n","Test Step: leave at (7, 5, 'door', False)\n","game end!\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0OuiOIGve9LY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640108901388,"user_tz":300,"elapsed":3302,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"2b526dd3-36cd-48e1-a5e1-ba089eb97d03"},"outputs":[{"output_type":"stream","name":"stdout","text":["eval 01: 14\n","eval 02: 14\n","eval 03: 14\n","eval 04: 14\n","eval 05: 14\n","eval 06: 14\n","eval 07: 14\n","eval 08: 14\n","eval 09: 14\n","eval 10: 14\n"]}],"source":["# part 1: TD Q-learning agent\n","\n","# init params\n","gamma = 0.1\n","N = np.zeros((288, 6))\n","Q = np.zeros((288, 6))\n","\n","num_evaluations = 10\n","time_to_completion = np.zeros((num_evaluations,))\n","\n","for k in np.arange(num_evaluations):\n","    for episode in np.arange(200):\n","        s = 0   # state with idx 0 is the starting state\n","        while True:\n","            # take a step, and exit if the game is over\n","            a = np.argmax([exp_func(N[s, i], Q[s, i]) for i in range(6)])\n","            sp, r = step(s, a)\n","            if sp == 'exit':\n","                break\n","            # lr function from lecture slides\n","            lr = 60 / (N[s, a] + 60)\n","            # TD update\n","            td = r + gamma * np.max(Q[sp]) - Q[s, a]\n","            Q[s, a] += lr * td\n","            # update frequency (exploration function)\n","            N[s, a] += 1\n","            # update state\n","            s = sp\n","        # print('ep %02d' % episode)\n","\n","    # evaluate the agent   \n","    s = 0\n","    step_count = 0\n","    while True:\n","        a = np.argmax([exp_func(N[s, i], Q[s, i]) for i in range(6)])\n","        s, r = step(s, a)\n","        step_count += 1\n","        if s == 'exit':\n","            break\n","    \n","    time_to_completion[k] = step_count\n","    print('eval %02d:' % (k+1), step_count)"]},{"cell_type":"code","source":["# plot eval steps for each epoch\n","plt.plot(time_to_completion)\n","plt.xlabel('epoch num')\n","plt.ylabel('total steps')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"1JNpoZ6pA8xL","executionInfo":{"status":"ok","timestamp":1640108902133,"user_tz":300,"elapsed":766,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"8f700c16-8ca6-4846-e454-98c5ec03f6ca"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'total steps')"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"3ZF_8jh9e9La"},"source":["#### Question 2. The parametric model (15pts)\n","\n","We now want to improve the generalization skills of our agent. We will encode our $Q$-table as a simple feedforward neural network. To avoid losing time with the implementation, we will rely on keras for the architecture (see below). We will consider a simple one hidden layer network that takes as input a vector encoding the current state of the environment (note that such a vector can be encoded in various ways, for example as a $5$-tuple including the cartesian coordinates of the agent and 3 variables indicating whether there is a door, skeleton,..) and returns as ouput the value associated with each of the actions. In other words we consider a parametric model $f_\\theta\\in \\mathbb{R}^{|a|}$ such that $[f_{\\theta}(s)]_a \\approx Q[s, a]$. Recall that the TD update for such a model is given\n","\n","\\begin{align}\n","\\theta_i &\\leftarrow \\theta_i + \\alpha \\left[R[s] + \\gamma \\max_{a'} \\hat{Q}_{\\theta}[s',a'] - \\hat{Q}[s,a]\\right]\\frac{\\partial \\hat{Q}_\\theta[s,a]}{\\partial \\theta_i} \n","\\end{align}\n","\n","- Start by updating the weights after each new step (in an SGD fashion) and evaluate the agent as before by interlacing your training with a couple of evaluation step where you should compute the time needed to exit with the kill completed. \n","\n","\n","- We will then be a little wiser and keep track of a short memory before updating the network. \n","\n","Consider the following steps :\n","\n","   - Initialize the memory to an empty vector\n","   \n","    \n","   - As before, we will consider a number of episodes. For each episode, we will consider a full simulation (until exiting with the kill). Again you should rely on some exploration function to alternate between exploration and exploitation (selection of the action corresponding to the maximum output of the network for the current state). \n","   \n","    \n","   - After each action, observe the reward and store the tuple $(s_t, a_t, r_t, s_{t+1})$ and store it in the memory. \n","   \n","    \n","   - Select a random minibatch from the memory and update the networks through a gradient update on the minibatch (mb) \n","    \n","    $$\\min_{\\theta_i} \\frac{1}{\\text{size}(\\text{mb})}\\sum_{t\\in \\text{mb}}\\left(R[s_t] + \\gamma \\max_{a_{t+1}} \\hat{Q}_{\\theta}[s_{t+1},a_{t+1}] - \\hat{Q}_\\theta[s_t,a_t]\\right)^2 $$\n","\n","\n","\n","- To see how to build a Feedforward neural network in keras, the simplest approach is to use the method 'add' such as in\n","\n","        model = keras.Sequential()\n","        model.add(layers.Dense(n, activation=\"relu\"))\n","\n","    you can see [here](https://keras.io/guides/sequential_model/) for more details.\n","    \n","    \n","    \n","- Once you have built the model, you need to select an optimizer before you can train it. This can be implemented through the ['compile method'](https://keras.io/api/optimizers/)\n","\n","          model.compile(optimizer='SGD',\n","          loss='mean_squared_error',\n","          metrics=['accuracy'])\n","          \n","- Finally to train the network, you can rely on the fit function (see also [here](https://keras.io/api/models/model_training_apis/))\n","\n","        model.fit(train_data, train_labels,epochs=5)\n","        \n","Note that Keras (when using 'compile' with the SGD option) relies on a default batch of size 32 so either you should specify a smaller batch size or you should make sure your memory is sufficiently big. Also note that as written above we will in fact do a coule of SGD iterations (one for each sample in the minibatch, repeated epochs times).  "]},{"cell_type":"code","source":["# convert a state idx to an embedding vector of size (1, 6)\n","# Note: for flags, True => 100, False => 1\n","def state_embedding(s, true_flag=100):\n","    x, y, cell, skeleton = idx_to_state[s]\n","    kill_flag = true_flag if not skeleton else 0\n","    door_flag = true_flag if cell == 'door' else 0\n","    cell_flag = true_flag if cell == 'nothing' else 0\n","    skeleton_flag = true_flag if cell == 'skeleton' else 0\n","    return np.array(\n","        [x, y, kill_flag, door_flag, cell_flag, skeleton_flag]\n","    ).reshape(1, 6)\n","\n","# convert the recoreds in buffer to (data, labels)\n","def convert(buffer, model, mode='q-table', gamma=0.1):\n","    data_list = []\n","    label_list = []\n","    for s, a, r, sp in buffer:\n","        if mode == 'q-table':\n","            label = Q[s, :].reshape(1, 6)\n","            s = state_embedding(s)\n","        else:\n","            # follow the question, use (r[s] + gamma * max(Q[s', a']))\n","            # Note: this is relatively slow\n","            s, sp = state_embedding(s), state_embedding(sp)\n","            label = model.predict(s)\n","            label[0, a] = r + gamma * np.max(model.predict(sp))\n","        data_list.append(s)\n","        label_list.append(label)\n","    return np.vstack(data_list), np.vstack(label_list)\n","\n","# init model\n","def get_model(linear=False, hidden=32, act='relu'):\n","    if linear:\n","        return Sequential([\n","            keras.Input(shape=(6,)),\n","            Dense(6) \n","        ])\n","    assert isinstance(hidden, int)\n","    return Sequential([\n","        keras.Input(shape=(6,)),\n","        Dense(hidden, activation=act),\n","        Dense(6) \n","    ])\n","\n","# init optimizer\n","def get_optim(optimizer, lr=0.01):\n","    assert optimizer in ('SGD', 'Adam')\n","    if optimizer == 'SGD':\n","        return keras.optimizers.SGD(learning_rate=lr)\n","    else:\n","        return keras.optimizers.Adam(learning_rate=lr)\n","\n","# test functions\n","optim = get_optim('SGD', lr=0.1)\n","model = get_model(linear=True)\n","model.compile(\n","    optimizer=optim,\n","    loss='mean_squared_error',\n","    metrics=['accuracy']\n",")\n","print('test embedding:', state_embedding(123), sep='\\n')\n","buffer = [(0, 0, 100, 2), (287, 5, -100, 2)]\n","print('test pred:', *convert(buffer, model), sep='\\n')"],"metadata":{"id":"Ots9cRaNNvsK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640108902291,"user_tz":300,"elapsed":170,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"47940883-9684-4043-a94a-d790d728a46d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["test embedding:\n","[[  3   2 100   0   0 100]]\n","test pred:\n","[[  0   0   0   0 100   0]\n"," [  7   5 100 100   0   0]]\n","[[ -3.          -1.11108011  -1.11108011  -3.         -11.\n","  -11.09994712]\n"," [ -1.          -3.          -3.          -1.         -11.\n","    0.        ]]\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BKYUuKKbe9Lc","executionInfo":{"status":"ok","timestamp":1640108902596,"user_tz":300,"elapsed":310,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}}},"outputs":[],"source":["# we may use the Q-table from part 1 to train the network\n","assert Q is not None\n","\n","door_pos = (7, 5)\n","skeleton_pos = (3, 3)\n","N = np.zeros((288, 6))\n","\n","def train(config, epoch=10, episode=200, max_iter=1000):\n","    optim = get_optim(**config['optim_kwargs'])\n","    model = get_model(**config['model_kwargs'])\n","    model.compile(\n","        optimizer=optim,\n","        loss='mean_squared_error',\n","        metrics=['accuracy']\n","    )\n","    # print(model.summary())\n","\n","    for k in range(epoch):\n","        for e in range(episode):\n","            s = 0\n","            buffer = []\n","            while True:\n","                if config['action_selection'] == 'q-table':\n","                    # generate training samples based on part 1\n","                    a = np.argmax([Q[s, i] for i in range(6)])\n","                else:\n","                    # generate training samples from scratch\n","                    # Note: this is really slow\n","                    A = model.predict(state_embedding(s))\n","                    a = np.argmax([exp_func(N[s, i], A[0, i]) for i in range(6)])\n","                # take s step, exit as needed\n","                sp, r = step(s, a)\n","                if sp == 'exit':\n","                    break\n","                # recard the sample\n","                buffer.append((s, a, r, sp))\n","                N[s, a] += 1\n","                # update state\n","                s = sp\n","            # fit the model using the sample\n","            # Note: keras split the data to batches automatically\n","            data, labels = convert(buffer, model, mode=config['label_mode'])\n","            model.fit(data, labels, epochs=5, verbose=config['verbose'])\n","            print('\\r epoch %02d, ep %02d.' % (k+1, e+1), end='')\n","\n","        if config['test_pred']:\n","            print('\\ntest pred:')\n","            print(model.predict(state_embedding(0)))\n","\n","        # evaluate the agent\n","        # Note: it seems that the agent cannot exit in all cases\n","        s = 0\n","        step_count = 0\n","        while True:\n","            A = model.predict(state_embedding(s))\n","            a = np.argmax([exp_func(N[s, i], A[0, i]) for i in range(6)])\n","            s, r = step(s, a)\n","            step_count += 1\n","            if (s == 'exit') or (step_count > max_iter):\n","                break\n","        if step_count > max_iter:\n","            print('max iter reached, eval fail.\\n')\n","            time_to_completion[k] = 1000\n","            # break\n","        else:\n","            time_to_completion[k] = step_count\n","            print('eval %02d:' % (k+1), step_count)"]},{"cell_type":"markdown","metadata":{"id":"rzLDX7uBe9Ld"},"source":["#### Question 3. (5pts) \n","\n","Compare and plot the improvement in the behavior of your agent for a couple of neural networks architectures. Don't make the network overly complicated. You can just try different activation functions with different numbers of units per layers (possibly also consider a _small_ number of hidden layers)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"XFA7UAR0e9Le","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640108902597,"user_tz":300,"elapsed":13,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"d55113c6-51f6-44c5-ef84-965c4a9b76bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["skip this cell\n"]}],"source":["\"\"\"\n","setting that follows the question instruction\n","    1) SGD optimizer (lr = 0.01)\n","    2) 2-layer MLP, with 32 hidden units, relu activation\n","    3) action selection: argmax(exp_func(N[s, i], A[0, i])), 0 <= i <= 5\n","    4) true label: r[s] + gamma * max(Q[s', a'])\n","\n","Note: This is extremely slow, please skip this cell while running.\n","\"\"\"\n","\n","# config = {\n","#     'optim_kwargs': {'optimizer': 'SGD', 'lr': 0.01},\n","#     'model_kwargs': {'linear': False, 'hidden': 32, 'act': 'relu'},\n","#     'action_selection': 'TD',\n","#     'label_mode': 'TD',\n","#     'test_pred': False,\n","#     'verbose': False\n","# }\n","\n","# train(config)\n","\n","print('skip this cell')"]},{"cell_type":"code","source":["\"\"\"\n","setting that follows the question instruction\n","    1) Adam optimizer (lr = 0.01)\n","    2) 2-layer MLP, with 32 hidden units, relu activation\n","    3) action selection: q-table from part 1\n","    4) true label: r[s] + gamma * max(Q[s', a'])\n","\n","Note: 1) A bit faster, but still cannot exit.\n","      2) Num of episode is quite small (only for a demonstration of speed).\n","\"\"\"\n","\n","config = {\n","    'optim_kwargs': {'optimizer': 'Adam', 'lr': 0.01},\n","    'model_kwargs': {'linear': False, 'hidden': 32, 'act': 'relu'},\n","    'action_selection': 'q-table',\n","    'label_mode': 'TD',\n","    'test_pred': True,\n","    'verbose': False\n","}\n","\n","train(config, epoch=2, episode=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ca-KT52iBxi_","executionInfo":{"status":"ok","timestamp":1640109319378,"user_tz":300,"elapsed":125613,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"f8f1f886-650a-4e49-c7b2-ca11ed07f20b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":[" epoch 01, ep 05.\n","test pred:\n","[[ 18.291338    2.4013455   2.0154176 -18.666304   -8.9192505  32.425068 ]]\n","max iter reached, eval fail.\n","\n"," epoch 02, ep 05.\n","test pred:\n","[[ 18.26525     1.7013525   2.0689435 -18.56452    -8.762165   32.433517 ]]\n","max iter reached, eval fail.\n","\n"]}]},{"cell_type":"code","source":["\"\"\"\n","setting that follows the question instruction\n","    1) Adam optimizer (lr = 0.01)\n","    2) 2-layer MLP, with 32 hidden units, relu activation\n","    3) action selection: q-table from part 1\n","    4) true label: q-table from part 1\n","\n","Note: 1) Much faster, but still cannot exit.\n","      2) This borrows the idea of Knowledge Distillation, where\n","         the q-table is the teacher model, the NN is the student model.\n","      3) You can set 'verbose' to True to observe a decrease of loss.\n","      4) I tried to modify hidden units and activation, not working either.\n","\n","Thoughts:\n","      1) NN may not be a proper model for this case.\n","      2) There should be better ways to generate the embedding vectors.\n","\"\"\"\n","\n","config = {\n","    'optim_kwargs': {'optimizer': 'Adam', 'lr': 0.01},\n","    'model_kwargs': {'linear': False, 'hidden': 32, 'act': 'relu'},\n","    'action_selection': 'q-table',\n","    'label_mode': 'q-table',\n","    'test_pred': True,\n","    'verbose': False\n","}\n","\n","train(config, epoch=10, episode=200)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aeeKpoJEsdn","executionInfo":{"status":"ok","timestamp":1640110027361,"user_tz":300,"elapsed":688921,"user":{"displayName":"Hammond Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOzSFkObCl-UMX25oW5lPfwwbD5InBzHMI_DJfSw=s64","userId":"10852634671235052019"}},"outputId":"60f99202-1970-4e46-833f-ce14127cb10b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":[" epoch 01, ep 200.\n","test pred:\n","[[ -2.0049984  -1.3957303  -1.1947311  -2.4602242 -11.020567  -11.126048 ]]\n","max iter reached, eval fail.\n","\n"," epoch 02, ep 200.\n","test pred:\n","[[ -2.2058663  -1.4894284  -1.1871046  -2.5957189 -11.056798  -11.140679 ]]\n","max iter reached, eval fail.\n","\n"," epoch 03, ep 200.\n","test pred:\n","[[ -2.3094728  -1.5417001  -1.1897585  -2.6783636 -11.040906  -11.119676 ]]\n","max iter reached, eval fail.\n","\n"," epoch 04, ep 200.\n","test pred:\n","[[ -2.3445756  -1.5308115  -1.1908485  -2.7007606 -11.006304  -11.10928  ]]\n","max iter reached, eval fail.\n","\n"," epoch 05, ep 200.\n","test pred:\n","[[ -2.3546264  -1.5285702  -1.1846836  -2.6937344 -11.003042  -11.10034  ]]\n","max iter reached, eval fail.\n","\n"," epoch 06, ep 200.\n","test pred:\n","[[ -2.3760288  -1.5278568  -1.2118956  -2.6886318 -10.98455   -11.107802 ]]\n","max iter reached, eval fail.\n","\n"," epoch 07, ep 200.\n","test pred:\n","[[ -2.3612297  -1.5229146  -1.1720834  -2.6913066 -10.999508  -11.094743 ]]\n","max iter reached, eval fail.\n","\n"," epoch 08, ep 200.\n","test pred:\n","[[ -2.343479   -1.4666141  -1.1000186  -2.7176805 -11.074131  -11.096479 ]]\n","max iter reached, eval fail.\n","\n"," epoch 09, ep 200.\n","test pred:\n","[[ -2.3226175  -1.3886662  -1.1577219  -2.7292168 -11.002538  -11.096994 ]]\n","max iter reached, eval fail.\n","\n"," epoch 10, ep 200.\n","test pred:\n","[[ -2.3175526  -1.4511393  -1.1664002  -2.707934  -10.986387  -11.075753 ]]\n","max iter reached, eval fail.\n","\n"]}]},{"cell_type":"markdown","source":["#### **End of Document**\n","\n","Have a nice day! :-)"],"metadata":{"id":"I9lMj7PSGNwn"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"AI_F2021_a4.ipynb","provenance":[{"file_id":"https://github.com/acosse/Artificial-Intelligence-Fall2021/blob/main/Assignments/Assignment4/CSCI-UA9472_Assignment4.ipynb","timestamp":1640048094174}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}